{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How cross entropy works.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdamadi/Today-I-learned/blob/master/How_cross_entropy_works.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcgLGbaY10jI",
        "colab_type": "text"
      },
      "source": [
        "This notebook breaks down how cross_entropy function (corresponding to CrossEntropyLoss used for classification) is implemented in pytorch, and how it is related to softmax, log_softmax, and nll (negative log-likelihood)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDbTY3h014Fc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klQ-8TCA2BKb",
        "colab_type": "code",
        "outputId": "df6f3061-9293-45a5-aa6d-8cc6a9b115a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "batch_size , n_classes  = 5, 3\n",
        "x = torch.randn(batch_size, n_classes)\n",
        "x.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86MjCCNT2UCN",
        "colab_type": "code",
        "outputId": "515806bf-4fc0-48b9-e725-76d927f71843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.3877, -1.2656, -0.9112],\n",
              "        [ 0.6011,  0.0092, -0.0982],\n",
              "        [-0.0674, -0.2059,  0.3410],\n",
              "        [ 1.3918,  0.2355,  0.4159],\n",
              "        [-0.6652, -1.6354, -0.9315]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMLYYloX3sEt",
        "colab_type": "text"
      },
      "source": [
        "`torch.randint` takes highest integer to be drawn from the distribution. `Size` is the output size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01PKiF7u7uYO",
        "colab_type": "text"
      },
      "source": [
        "```Softmax Regression = Multi-class Logistic Regression```\n",
        "\n",
        "**Multi-class classification** is a generalization of **logistic regression** wherein we are dealing with binary classification. Note that in both cases we assume the classes are mutually exclusive.\n",
        "\n",
        "**Logistic regression** loss function uses the output of a neural network ($\\hat{y}$) to calculate the loss of the network. To that end, the result of last layer ($z^L$) is passed in sigmoid function defined as $\\sigma({z^L})=\\frac{e^{z^L}}{1+ e^{z^L}}=\\hat{y}$ where $z^L \\in \\mathbb{R}$ is the linear output of the last layer, $y$ is the ground truth, and the loss is defined as\n",
        "$$\n",
        "\\ell(y, \\hat{y}) = -(y\\log\\hat{y} + (1-y)\\log(1-\\hat{y}))\n",
        "$$\n",
        "\n",
        "On the other hand **multi-class classification** loss recieves a vector, i.e., $\\hat{\\textbf{y}}$ to calculate the loss; $\\hat{\\textbf{y}}$ is obtained by feeding the linear output of the last layer $\\textbf{z}^L$ to the softmax function. Softmax is defined as $\\text{softmax}(\\textbf{z}^L)=\\frac{e^{{\\textbf{z}}^L_i}}{\\sum_{i=1}^{n}e^{{\\textbf{z}}^L_i}}=\\hat{\\textbf{y}_i}$ where $\\hat{\\textbf{y}} \\in \\mathbb{R}^n$ is the output of the network and $\\textbf{y}$ is the ground truth vector. Therefore, the loss function would be\n",
        "$$\n",
        "\\ell(\\textbf{y}, \\hat{\\textbf{y}}) = -\\frac{1}{m}\\sum_{k=1}^{n}{\\textbf{y}}_k\\log(\\hat{\\textbf{y}}_k)\n",
        "$$\n",
        "The above loss function is called **cross entropy** loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCojRayh2Y_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target = torch.randint(n_classes, size = (batch_size,), dtype = torch.long)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-OHJxq_4LFD",
        "colab_type": "code",
        "outputId": "e1c80f9d-046c-4605-c69c-80c553b950df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "target"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 0, 1, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUAO9e8OLsvx",
        "colab_type": "code",
        "outputId": "0f99c703-84d9-4b73-f32f-1d7f1e90cf34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "x.exp()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4.0058, 0.2821, 0.4020],\n",
              "        [1.8241, 1.0093, 0.9065],\n",
              "        [0.9349, 0.8139, 1.4064],\n",
              "        [4.0222, 1.2656, 1.5157],\n",
              "        [0.5142, 0.1949, 0.3940]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHAi6DL0KJNq",
        "colab_type": "code",
        "outputId": "bc9105ea-1144-4e7e-dc79-af505797161b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "x.exp().sum(-1).unsqueeze(-1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4.6899],\n",
              "        [3.7398],\n",
              "        [3.1551],\n",
              "        [6.8035],\n",
              "        [1.1030]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmGjso1cKozB",
        "colab_type": "code",
        "outputId": "c1ab4488-dd8f-4d39-92e2-145b7f8c9494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "x.exp()/x.exp().sum(-1).unsqueeze(-1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8541, 0.0601, 0.0857],\n",
              "        [0.4877, 0.2699, 0.2424],\n",
              "        [0.2963, 0.2580, 0.4457],\n",
              "        [0.5912, 0.1860, 0.2228],\n",
              "        [0.4662, 0.1767, 0.3572]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92dFB7ZIMXc9",
        "colab_type": "text"
      },
      "source": [
        "#Building the loss function applying $\\text{softmax}$ and then negative log likelihood\n",
        "\n",
        "This version is most similar to the math formula, but not numerically stable.\n",
        "\n",
        "In the following, we first calculate the vector value of $\\hat{\\textbf{y}}$ after applying softmax function on $\\textbf{z}^L$, then, based on the ground truth vector which has only one $1$, a particular elementn of $\\hat{\\textbf{y}}$ is picked for one data point. Finally, the $\\log$ function is applied. Since we are calculating multiple data points, we take average over all the data samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZeKqKwkMHGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "  return x.exp() / (x.exp().sum(-1)).unsqueeze(-1)\n",
        "def nl(input, target):\n",
        "  return -input[range(target.shape[0]), target].log().mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3R9bEu4PVLy",
        "colab_type": "code",
        "outputId": "b52b49a0-65bd-4513-fe44-11abf63647ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "target.shape[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ymv80iQZPVqU",
        "colab_type": "code",
        "outputId": "a87d3885-2816-4ddb-da56-f97e08f0afa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "range(target.shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "range(0, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuJre6T0Pa8R",
        "colab_type": "code",
        "outputId": "a3f2b9f8-219d-4d9e-ffdc-344052adfaf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "pred = softmax(x)\n",
        "pred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8541, 0.0601, 0.0857],\n",
              "        [0.4877, 0.2699, 0.2424],\n",
              "        [0.2963, 0.2580, 0.4457],\n",
              "        [0.5912, 0.1860, 0.2228],\n",
              "        [0.4662, 0.1767, 0.3572]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OB-gx_PExBz",
        "colab_type": "code",
        "outputId": "0cfcdba5-981c-4497-ce86-92dae6b42253",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pred[range(target.shape[0]), target]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0857, 0.4877, 0.2580, 0.5912, 0.1767])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6saET5bFVI-",
        "colab_type": "code",
        "outputId": "1dc66c4a-157b-4213-a465-12d289490670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "-pred[range(target.shape[0]), target].log()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.4566, 0.7180, 1.3550, 0.5256, 1.7335])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSdcnSJMPyEM",
        "colab_type": "code",
        "outputId": "63aaac87-8a7a-4806-b0a2-3049c506ea27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "-pred[range(target.shape[0]), target].log().mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.3577)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snNCe9dRP_Nv",
        "colab_type": "code",
        "outputId": "ab6dfdb9-6798-4b28-d177-26d661d62add",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pred = softmax(x)\n",
        "loss=nl(pred, target)\n",
        "loss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.3577)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwfIiMAjGeDv",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f30q6yL47nmC",
        "colab_type": "text"
      },
      "source": [
        "#Building the loss function by applying $\\log$ and then calculating softmax.\n",
        "\n",
        "While mathematically equivalent to log(softmax(x)), doing these two operations separately is slower, and numerically unstable. This function uses an alternative formulation to compute the output and gradient correctly.\n",
        "\n",
        "Since we want just one element of $\\hat{\\textbf{y}}$ based on grand truth,let's say $i$-th element, we can first apply softmax and the take the log. $\\log(\\frac{e^{\\textbf{z}_i^L}}{\\sum_ie^{\\textbf{z}_k^L}})=\\textbf{z}_i^L-\\log(\\sum_ie^{\\textbf{z}_k^L})$\n",
        "\n",
        "```python\n",
        "x - x.exp().sum(-1).log().unsqueeze(-1)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMWvNluHWyFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = nn.LogSoftmax()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smEiTudaIfeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = torch.randn(2, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGjkjYWmI_ot",
        "colab_type": "code",
        "outputId": "e075e0a2-4cda-4959-a378-b0473fb32eda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "m(input)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.7986, -0.8772, -0.6479],\n",
              "        [-1.7172, -0.6639, -1.1855]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPPpZnMWIljY",
        "colab_type": "code",
        "outputId": "b51e35f5-4e02-4c9b-b196-d75a12d24280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "input"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.6471,  1.2743,  1.5036],\n",
              "        [-0.6790,  0.3743, -0.1474]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ6JJf38I8sY",
        "colab_type": "code",
        "outputId": "a6083066-0ecb-43e1-c5f7-a4eeb3b2d09b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "input.exp()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5236, 3.5764, 4.4981],\n",
              "        [0.5071, 1.4539, 0.8630]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBjAwe4LIm2c",
        "colab_type": "code",
        "outputId": "5442e88a-abf9-4b71-ea5d-343e4808293d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "input.exp().sum(-1).unsqueeze(-1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[8.5980],\n",
              "        [2.8240]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjTHjnqwIxtp",
        "colab_type": "code",
        "outputId": "af1a2fe1-efb5-4291-fc32-c61a9ad793ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "input.exp()/input.exp().sum(-1).unsqueeze(-1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0609, 0.4160, 0.5232],\n",
              "        [0.1796, 0.5148, 0.3056]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv4-5sOdJUfV",
        "colab_type": "code",
        "outputId": "ca861446-d4f7-48a5-b5c4-e51a8a9d1e95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "(input.exp()/input.exp().sum(-1).unsqueeze(-1)).log()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.7986, -0.8772, -0.6479],\n",
              "        [-1.7172, -0.6639, -1.1855]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjf2tIPHJfF7",
        "colab_type": "code",
        "outputId": "52dac605-1ca2-416e-def4-188c6be4e1f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def log_softmax(x): \n",
        "  return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
        "def nll(input, target): \n",
        "  return -input[range(target.shape[0]), target].mean()\n",
        "\n",
        "pred = log_softmax(x)\n",
        "loss = nll(pred, target)\n",
        "loss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.3577)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ezZWiNmPgap",
        "colab_type": "text"
      },
      "source": [
        "#Using pytorch functional module to calculate the loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X0KghxTPfDU",
        "colab_type": "code",
        "outputId": "3ce04df2-4e2f-4f1b-ffd0-e1bcaec1eb4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pred = F.log_softmax(x, dim=-1)\n",
        "loss = F.nll_loss(pred, target)\n",
        "loss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.3577)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-uBc4myTMGr",
        "colab_type": "text"
      },
      "source": [
        "#Finding the loss in one shot using cross entropy loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCYNxc00S6wo",
        "colab_type": "code",
        "outputId": "db568dc4-28f4-4932-9c72-bb84dbb18eca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "F.cross_entropy(x, target)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.3577)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBK0gHE6TsCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}